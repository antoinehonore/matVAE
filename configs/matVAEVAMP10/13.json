{
   "training_parameters": {
      "num_training_steps": 300000,
      "learning_rate": 8e-05,
      "betas": [
         0.9,
         0.999
      ],
      "batch_size": 256,
      "model_mixing": 0,
      "annealing_warm_up": 0,
      "kl_latent_scale": 1,
      "kl_global_params_scale": 0,
      "gradient_noise_std": 0,
      "gradient_clip_norm": 0,
      "l2_regularization": 0,
      "use_lr_scheduler": false,
      "use_validation_set": true,
      "validation_set_pct": 0.2,
      "validation_freq": 1000,
      "log_training_info": true,
      "log_training_freq": 1000,
      "log_grad_every": 2000,
      "data_flip_p": 0,
      "save_model_params_freq": 500000,
      "loss_fun": "x_CE",
      "check_val_every_n_steps": 2000,
      "min_delta": 1e-06,
      "patience_n_steps": 1000000,
      "latent_to_dms_target": "target",
      "use_dms": 1,
      "use_dms_only": false,
      "use_dms_factor": 0,
      "latent_to_dms_loss": "mse",
      "plot_batch_index": -1,
      "plot_elbos": false
   },
   "data_parameters": {
      "msa_list": "npc1_human.csv",
      "testprot": [],
      "training_data": "msa",
      "factorisation_type": "none",
      "factorisation_opts": "default",
      "positional_encoding_type": "none",
      "positional_encoding_opts": "none",
      "missing_target_distribution": "default",
      "encoding_type": "none",
      "features": "onehot"
   },
   "model_parameters": {
      "vae_enc_layers": [
         1000,
         300
      ],
      "vae_z_dim": 50,
      "input_size": 0,
      "layernorm": true,
      "skipconnections": true,
      "skiptemperature": true,
      "vae_dropout_p": 0,
      "vae_decoder_bayesian": false,
      "vae_activation": "relu",
      "vae_stochastic_latent": true,
      "output_scaler": 1,
      "output_function": "identity",
      "reconstruction_error": "x_CE",
      "vae_num_prior_components": 10,
      "vae_prior_type": "vampgumbel",
      "vae_prior_temperature_scaler": 1,
      "vae_prior_init_scaler": 1,
      "vae_decoder_convolve_output": 0,
      "vae_decoder_include_sparsity": 0,
      "vae_latent_n_layers": 1,
      "vae_latent_activation": "relu",
      "model_type": "transformers+Lmax",
      "include_temperature_scaler": 3,
      "transformer_dropout_p": 0,
      "transformer_activation": "relu",
      "transformer_num_layers": 3,
      "transformer_key_dim": 20,
      "transformer_embed_dim": 20,
      "transformer_use_structure": 7,
      "transformer_attnffn_num_layers": 2,
      "transformer_num_heads": 1,
      "transformer_kernel_size": 1,
      "variable_length_n_layers": 1,
      "variable_length_Lmax": 4000,
      "variable_length_H": 200,
      "variable_length_dropout_p": 0,
      "variable_length_activation": "relu",
      "latent_to_dms_activation": "relu",
      "latent_to_dms_layer_sizes": [
         50,
         25
      ],
      "latent_to_dms_detach": false,
      "latent_to_dms_sigmoid": false
   }
}